{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5_oa5cohrNu"
      },
      "outputs": [],
      "source": [
        "EDA - 1 Title: Bike Details Dataset\n",
        "\n",
        "1.What is the range of selling prices in the dataset?\n",
        "To find the range of selling prices in a dataset using Python, you can follow these steps:\n",
        "\n",
        "Load the dataset into a DataFrame.\n",
        "Extract the column containing the selling prices.\n",
        "Find the minimum and maximum values in that column.\n",
        "Calculate the range as the difference between the maximum and minimum values.\n",
        "Hereâ€™s a Python code example:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for selling prices is named 'selling_price'\n",
        "selling_prices = df['selling_price']\n",
        "\n",
        "# Calculate the range\n",
        "min_price = selling_prices.min()\n",
        "max_price = selling_prices.max()\n",
        "price_range = max_price - min_price\n",
        "\n",
        "print(f\"The range of selling prices is: {price_range}\")\n",
        "Replace 'selling_price' with the actual column name in your dataset that represents selling prices. If you have the dataset, you can share it or describe the structure so I can tailor the code to your specific needs!\n",
        "\n",
        "2. What is the median selling price for bikes in the dataset?\n",
        "To calculate the median selling price for bikes in a dataset using Python, you can use the median() function from the pandas library. Here's the code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for selling prices is named 'selling_price'\n",
        "median_price = df['selling_price'].median()\n",
        "\n",
        "print(f\"The median selling price for bikes is: {median_price}\")\n",
        "Explanation:\n",
        "df['selling_price']: Selects the column containing the selling prices.\n",
        ".median(): Calculates the median value of the column.\n",
        "Replace 'selling_price' with the actual column name in your dataset that represents selling prices. If you share more details about your dataset, I can provide additional guidance!\n",
        "\n",
        "3. What is the most common seller type?\n",
        "To determine the most common seller type in a dataset using Python, you can use the mode() function from the pandas library. Here's the code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for seller type is named 'seller_type'\n",
        "most_common_seller_type = df['seller_type'].mode()[0]\n",
        "\n",
        "print(f\"The most common seller type is: {most_common_seller_type}\")\n",
        "Explanation:\n",
        "df['seller_type']: Selects the column containing seller types.\n",
        ".mode(): Finds the most frequent value in the column.\n",
        "[0]: Extracts the first mode in case of ties (multiple modes).\n",
        "Replace 'seller_type' with the actual column name for seller type in your dataset. If the dataset structure is unclear, feel free to provide more details, and I'll adjust the code accordingly!\n",
        "\n",
        " 4.How many bikes have driven more than 50,000 kilometers?\n",
        " To find how many bikes have driven more than 50,000 kilometers, you can use the pandas library to filter the dataset based on the mileage column. Here's the code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for kilometers driven is named 'km_driven'\n",
        "# Filter the dataset for bikes with mileage greater than 50,000\n",
        "bikes_over_50k = df[df['km_driven'] > 50000]\n",
        "\n",
        "# Count the number of such bikes\n",
        "count_bikes = bikes_over_50k.shape[0]\n",
        "\n",
        "print(f\"The number of bikes that have driven more than 50,000 kilometers is: {count_bikes}\")\n",
        "Explanation:\n",
        "df['km_driven']: Selects the column for kilometers driven.\n",
        "df[df['km_driven'] > 50000]: Filters the rows where km_driven is greater than 50,000.\n",
        ".shape[0]: Returns the number of rows in the filtered DataFrame, which corresponds to the count of bikes.\n",
        "Replace 'km_driven' with the actual column name for kilometers driven in your dataset. Let me know if you need further clarification!\n",
        "\n",
        "5. What is the average km_driven value for each ownership type?\n",
        "To calculate the average km_driven value for each ownership type, you can use the groupby function in pandas along with the mean() function. Here's the code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for kilometers driven is named 'km_driven' and ownership type is named 'ownership'\n",
        "average_km_by_ownership = df.groupby('ownership')['km_driven'].mean()\n",
        "\n",
        "print(\"The average km_driven value for each ownership type:\")\n",
        "print(average_km_by_ownership)\n",
        "Explanation:\n",
        "df.groupby('ownership'): Groups the dataset by the ownership column.\n",
        "['km_driven'].mean(): Calculates the mean of the km_driven column for each ownership group.\n",
        "Replace 'km_driven' and 'ownership' with the actual column names in your dataset. The result will be a series showing the average km_driven for each ownership type. Let me know if you need further assistance!\n",
        "\n",
        "To calculate the proportion of bikes from the year 2015 or older, you can use the following Python code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for the year is named 'year'\n",
        "# Filter bikes from the year 2015 or older\n",
        "bikes_2015_or_older = df[df['year'] <= 2015]\n",
        "\n",
        "# Calculate the proportion\n",
        "proportion = len(bikes_2015_or_older) / len(df)\n",
        "\n",
        "print(f\"The proportion of bikes from the year 2015 or older is: {proportion:.2%}\")\n",
        "Explanation:\n",
        "df['year'] <= 2015: Filters rows where the bike year is 2015 or older.\n",
        "len(bikes_2015_or_older): Counts the number of bikes from 2015 or older.\n",
        "len(df): Counts the total number of bikes in the dataset.\n",
        "proportion: Divides the count of bikes from 2015 or older by the total count.\n",
        ":.2%: Formats the result as a percentage with two decimal places.\n",
        "Replace 'year' with the actual column name for the manufacturing year in your dataset. Let me know if you need further clarification!\n",
        "\n",
        "7. What is the trend of missing values across the dataset?\n",
        "To identify the trend of missing values across the dataset, you can calculate the number or proportion of missing values for each column. Here's how you can do it in Python:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Calculate the number of missing values for each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Calculate the proportion of missing values for each column\n",
        "missing_proportion = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "# Combine the results into a DataFrame for better visualization\n",
        "missing_data_trend = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Missing Proportion (%)': missing_proportion\n",
        "}).sort_values(by='Missing Values', ascending=False)\n",
        "\n",
        "print(\"Trend of missing values across the dataset:\")\n",
        "print(missing_data_trend)\n",
        "Explanation:\n",
        "df.isnull().sum(): Counts the number of missing values for each column.\n",
        "df.isnull().sum() / len(df): Calculates the proportion of missing values for each column.\n",
        "pd.DataFrame: Combines the missing values and proportions into a DataFrame for a clear overview.\n",
        "sort_values(by='Missing Values', ascending=False): Sorts the columns by the number of missing values in descending order.\n",
        "This will display the trend of missing values across the dataset. Let me know if you need further clarification!\n",
        "\n",
        "8. What is the highest ex_showroom_price recorded, and for which bike?\n",
        "To find the highest ex_showroom_price and the bike associated with it, you can use the following Python code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the columns for price and bike model are named 'ex_showroom_price' and 'bike_name'\n",
        "# Find the row with the highest ex_showroom_price\n",
        "highest_price_row = df.loc[df['ex_showroom_price'].idxmax()]\n",
        "\n",
        "# Extract the details\n",
        "highest_price = highest_price_row['ex_showroom_price']\n",
        "bike_name = highest_price_row['bike_name']\n",
        "\n",
        "print(f\"The highest ex_showroom_price is {highest_price}, and it is recorded for the bike: {bike_name}\")\n",
        "Explanation:\n",
        "df['ex_showroom_price'].idxmax(): Finds the index of the row with the maximum ex_showroom_price.\n",
        "df.loc[]: Retrieves the row at the specified index.\n",
        "highest_price_row['ex_showroom_price']: Extracts the highest price value.\n",
        "highest_price_row['bike_name']: Extracts the name of the bike associated with the highest price.\n",
        "Replace 'ex_showroom_price' and 'bike_name' with the actual column names in your dataset. Let me know if you need further help!\n",
        "\n",
        "9.What is the total number of bikes listed by each seller type?\n",
        "To calculate the total number of bikes listed by each seller type, you can use the groupby function in pandas along with the count() function. Here's the code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for seller type is named 'seller_type'\n",
        "bikes_by_seller_type = df.groupby('seller_type').size()\n",
        "\n",
        "print(\"Total number of bikes listed by each seller type:\")\n",
        "print(bikes_by_seller_type)\n",
        "Explanation:\n",
        "df.groupby('seller_type'): Groups the dataset by the seller_type column.\n",
        ".size(): Counts the number of rows in each group.\n",
        "The output will show the total number of bikes listed for each seller type. Replace 'seller_type' with the actual column name in your dataset if necessary. Let me know if you have any questions!\n",
        "\n",
        "10. What is the relationship between selling_price and km_driven for first-owner bikes?\n",
        "To explore the relationship between selling_price and km_driven for first-owner bikes, you can filter the dataset for first-owner bikes and then visualize the relationship using a scatter plot. You can also calculate the correlation to quantify the relationship.\n",
        "\n",
        "Hereâ€™s the Python code for both:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for ownership type is 'ownership' and 'first-owner' represents the first owner\n",
        "# Filter the dataset for first-owner bikes\n",
        "first_owner_bikes = df[df['ownership'] == 'first-owner']\n",
        "\n",
        "# Plot the relationship between selling_price and km_driven\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=first_owner_bikes, x='km_driven', y='selling_price')\n",
        "plt.title('Relationship between Selling Price and Km Driven for First-Owner Bikes')\n",
        "plt.xlabel('Kilometers Driven')\n",
        "plt.ylabel('Selling Price')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the correlation between selling_price and km_driven for first-owner bikes\n",
        "correlation = first_owner_bikes['selling_price'].corr(first_owner_bikes['km_driven'])\n",
        "\n",
        "print(f\"The correlation between selling_price and km_driven for first-owner bikes is: {correlation}\")\n",
        "Explanation:\n",
        "Filtering first-owner bikes: df[df['ownership'] == 'first-owner'] filters the rows where the ownership column is equal to 'first-owner'.\n",
        "Visualization: sns.scatterplot() creates a scatter plot to visualize the relationship between selling_price and km_driven.\n",
        "Correlation: corr() calculates the Pearson correlation between selling_price and km_driven, which tells you how strongly the two variables are related.\n",
        "Ensure to replace 'ownership', 'selling_price', and 'km_driven' with the actual column names in your dataset. The scatter plot will give you a visual representation, and the correlation will provide a numerical measure of the relationship.\n",
        "\n",
        "To identify and remove outliers in the km_driven column using the IQR (Interquartile Range) method, follow these steps:\n",
        "\n",
        "Calculate the first quartile (Q1) and the third quartile (Q3).\n",
        "Compute the IQR by subtracting Q1 from Q3.\n",
        "Define the lower and upper bounds for outliers (typically 1.5 times the IQR).\n",
        "Filter the dataset to remove rows that fall outside these bounds.\n",
        "Hereâ€™s the Python code to perform this:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for kilometers driven is 'km_driven'\n",
        "# Calculate the first quartile (Q1) and the third quartile (Q3)\n",
        "Q1 = df['km_driven'].quantile(0.25)\n",
        "Q3 = df['km_driven'].quantile(0.75)\n",
        "\n",
        "# Calculate the IQR (Interquartile Range)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the lower and upper bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter the dataset to remove outliers\n",
        "df_cleaned = df[(df['km_driven'] >= lower_bound) & (df['km_driven'] <= upper_bound)]\n",
        "\n",
        "# Output the number of rows before and after removing outliers\n",
        "print(f\"Original dataset size: {df.shape[0]}\")\n",
        "print(f\"Dataset size after removing outliers: {df_cleaned.shape[0]}\")\n",
        "Explanation:\n",
        "Q1 and Q3: These are the first and third quartiles, representing the 25th and 75th percentiles of the data.\n",
        "IQR: The Interquartile Range is the difference between Q3 and Q1.\n",
        "Lower and upper bounds: Outliers are defined as values that are below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n",
        "Filtering the dataset: The dataset is filtered to include only rows where the km_driven values lie within the bounds.\n",
        "This code will remove the outliers from the km_driven column and print the sizes of the dataset before and after removing the outliers. Replace 'km_driven' with the actual column name for kilometers driven in your dataset.\n",
        "\n",
        "11. Identify and remove outliers in the km_driven column using the IQR method?\n",
        "To identify and remove outliers in the km_driven column using the IQR (Interquartile Range) method, follow these steps:\n",
        "\n",
        "Calculate the first quartile (Q1) and the third quartile (Q3).\n",
        "Compute the IQR by subtracting Q1 from Q3.\n",
        "Define the lower and upper bounds for outliers (typically 1.5 times the IQR).\n",
        "Filter the dataset to remove rows that fall outside these bounds.\n",
        "Hereâ€™s the Python code to perform this:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the column for kilometers driven is 'km_driven'\n",
        "# Calculate the first quartile (Q1) and the third quartile (Q3)\n",
        "Q1 = df['km_driven'].quantile(0.25)\n",
        "Q3 = df['km_driven'].quantile(0.75)\n",
        "\n",
        "# Calculate the IQR (Interquartile Range)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the lower and upper bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter the dataset to remove outliers\n",
        "df_cleaned = df[(df['km_driven'] >= lower_bound) & (df['km_driven'] <= upper_bound)]\n",
        "\n",
        "# Output the number of rows before and after removing outliers\n",
        "print(f\"Original dataset size: {df.shape[0]}\")\n",
        "print(f\"Dataset size after removing outliers: {df_cleaned.shape[0]}\")\n",
        "Explanation:\n",
        "Q1 and Q3: These are the first and third quartiles, representing the 25th and 75th percentiles of the data.\n",
        "IQR: The Interquartile Range is the difference between Q3 and Q1.\n",
        "Lower and upper bounds: Outliers are defined as values that are below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n",
        "Filtering the dataset: The dataset is filtered to include only rows where the km_driven values lie within the bounds.\n",
        "This code will remove the outliers from the km_driven column and print the sizes of the dataset before and after removing the outliers. Replace 'km_driven' with the actual column name for kilometers driven in your dataset.\n",
        "\n",
        "To perform a bivariate analysis to visualize the relationship between year and selling_price, you can use a scatter plot, box plot, or line plot. Hereâ€™s an example using a scatter plot and a box plot to visualize the relationship:\n",
        "\n",
        "Using Scatter Plot\n",
        "A scatter plot can help visualize the correlation between the two variables.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the columns are 'year' and 'selling_price'\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot to visualize the relationship\n",
        "sns.scatterplot(data=df, x='year', y='selling_price')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Relationship between Year and Selling Price')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Selling Price')\n",
        "\n",
        "plt.show()\n",
        "Using Box Plot\n",
        "A box plot can help visualize the distribution of selling_price for each year and identify any trends or patterns.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Box plot to visualize the distribution of selling_price for each year\n",
        "sns.boxplot(data=df, x='year', y='selling_price')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Distribution of Selling Price by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Selling Price')\n",
        "\n",
        "plt.show()\n",
        "Explanation:\n",
        "Scatter Plot: sns.scatterplot() is used to plot year on the x-axis and selling_price on the y-axis. This is useful to see if there is a linear relationship between these two variables.\n",
        "Box Plot: sns.boxplot() helps in visualizing the distribution of selling_price for each year. It shows the median, quartiles, and potential outliers.\n",
        "Make sure to replace 'year' and 'selling_price' with the actual column names in your dataset. This will help you visualize how selling_price changes with the year. Let me know if you need further help!\n",
        "\n",
        "13. What is the average depreciation in selling price based on the bike's age (current year - manufacturing year)?\n",
        "To calculate the average depreciation in selling price based on the bike's age (current year - manufacturing year), you can follow these steps:\n",
        "\n",
        "Compute the age of each bike using the formula current_year - manufacturing_year.\n",
        "Calculate the depreciation, which is the difference between the original selling price and the current selling price.\n",
        "Group the data by the bike's age and calculate the average depreciation for each age group.\n",
        "Hereâ€™s the Python code to do this:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the columns are 'year' for manufacturing year and 'selling_price' for the current selling price\n",
        "current_year = 2024  # You can use the current year dynamically using datetime if needed\n",
        "df['age'] = current_year - df['year']  # Calculate the age of each bike\n",
        "\n",
        "# Assuming the original selling price is in a column named 'original_selling_price'\n",
        "# Calculate the depreciation (difference between original selling price and current selling price)\n",
        "df['depreciation'] = df['original_selling_price'] - df['selling_price']\n",
        "\n",
        "# Group by the bike's age and calculate the average depreciation\n",
        "average_depreciation_by_age = df.groupby('age')['depreciation'].mean()\n",
        "\n",
        "# Display the results\n",
        "print(\"Average depreciation in selling price based on the bike's age:\")\n",
        "print(average_depreciation_by_age)\n",
        "Explanation:\n",
        "Age Calculation: The age of each bike is computed by subtracting the manufacturing year (year) from the current year (current_year).\n",
        "Depreciation Calculation: Depreciation is calculated as the difference between the original selling price (original_selling_price) and the current selling price (selling_price).\n",
        "Grouping by Age: The dataset is grouped by the bikeâ€™s age, and the mean depreciation is calculated for each age group.\n",
        "Make sure that the column names year, selling_price, and original_selling_price match the actual column names in your dataset. If you need further customization or have more specific requirements, let me know!\n",
        "\n",
        "To find which bike names are priced significantly above the average price for their manufacturing year, you can follow these steps:\n",
        "\n",
        "Calculate the average selling price for each manufacturing year.\n",
        "Determine a threshold above which a bike's price is considered significantly higher than the average price (e.g., 1 standard deviation or a custom threshold).\n",
        "Identify bikes whose selling prices exceed this threshold for their respective manufacturing year.\n",
        "Here's the Python code to achieve this:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Assuming the columns are 'year' for manufacturing year and 'selling_price' for current selling price\n",
        "# Calculate the average selling price for each manufacturing year\n",
        "avg_price_by_year = df.groupby('year')['selling_price'].mean()\n",
        "\n",
        "# Calculate the standard deviation for each manufacturing year (optional, for a more flexible threshold)\n",
        "std_price_by_year = df.groupby('year')['selling_price'].std()\n",
        "\n",
        "# Define a threshold to consider a bike's price as \"significantly above average\"\n",
        "# Example: Consider a price as significantly above average if it's greater than the average + 1 standard deviation\n",
        "threshold = avg_price_by_year + std_price_by_year\n",
        "\n",
        "# Find bikes priced above the threshold\n",
        "above_avg_bikes = df[df.apply(lambda row: row['selling_price'] > threshold[row['year']], axis=1)]\n",
        "\n",
        "# Get the bike names and corresponding prices\n",
        "significantly_above_avg_bikes = above_avg_bikes[['bike_name', 'year', 'selling_price']]\n",
        "\n",
        "print(\"Bikes priced significantly above the average price for their manufacturing year:\")\n",
        "print(significantly_above_avg_bikes)\n",
        "Explanation:\n",
        "avg_price_by_year: The average selling price is computed for each manufacturing year.\n",
        "std_price_by_year: The standard deviation of selling prices is calculated for each manufacturing year (optional but provides flexibility for thresholds).\n",
        "threshold: The threshold is defined as the average selling price plus one standard deviation. You can adjust this based on your criteria for \"significantly above average.\"\n",
        "above_avg_bikes: Filters bikes whose selling price is greater than the threshold for their respective year.\n",
        "significantly_above_avg_bikes: Displays the bike names, manufacturing year, and selling price for bikes that meet the condition.\n",
        "Make sure to replace 'year', 'selling_price', and 'bike_name' with the actual column names in your dataset. If you have other criteria for identifying \"significantly above average,\" you can adjust the threshold calculation accordingly. Let me know if you need further help!\n",
        "\n",
        "To develop a correlation matrix for numeric columns and visualize it using a heatmap, you can use pandas to calculate the correlation matrix and seaborn and matplotlib to visualize it. Here's the Python code for this:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'bike_details.csv' with your actual file path)\n",
        "df = pd.read_csv('bike_details.csv')\n",
        "\n",
        "# Calculate the correlation matrix for numeric columns\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "# Set plot title\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n",
        "Explanation:\n",
        "df.corr(): This function calculates the correlation matrix for all numeric columns in the dataset.\n",
        "sns.heatmap(): Visualizes the correlation matrix as a heatmap.\n",
        "annot=True: Annotates each cell with the numeric value.\n",
        "cmap='coolwarm': Specifies the color map for the heatmap (you can choose other color maps like 'viridis', 'plasma', etc.).\n",
        "fmt='.2f': Formats the annotated values to 2 decimal places.\n",
        "linewidths=0.5: Adds a border between cells.\n",
        "plt.title(): Adds a title to the heatmap.\n",
        "plt.show(): Displays the heatmap.\n",
        "This code will give you a heatmap that shows the correlation between all numeric columns in the dataset. If you have specific columns or a smaller subset you'd like to analyze, you can filter the DataFrame before calculating the correlation matrix.\n",
        "\n",
        "EDA - 2\n",
        "Title: Car Sale Dataset\n",
        " 1.What is the average selling price of cars for each dealer, and how does it compare across different dealers?\n",
        "To calculate the average selling price of cars for each dealer and compare it across different dealers, you can use the following steps in Python:\n",
        "\n",
        "Group the dataset by the dealer.\n",
        "Calculate the average selling price for each dealer.\n",
        "Visualize the comparison across dealers (e.g., using a bar chart).\n",
        "Here's the Python code to perform this analysis:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the column for dealer is 'dealer' and the column for selling price is 'selling_price'\n",
        "# Calculate the average selling price for each dealer\n",
        "average_price_by_dealer = df.groupby('dealer')['selling_price'].mean()\n",
        "\n",
        "# Print the average selling prices by dealer\n",
        "print(\"Average selling price for each dealer:\")\n",
        "print(average_price_by_dealer)\n",
        "\n",
        "# Visualize the average selling price across different dealers using a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "average_price_by_dealer.plot(kind='bar', color='skyblue')\n",
        "plt.title('Average Selling Price by Dealer')\n",
        "plt.xlabel('Dealer')\n",
        "plt.ylabel('Average Selling Price')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "Explanation:\n",
        "df.groupby('dealer')['selling_price'].mean(): This groups the dataset by the dealer column and calculates the average selling price for each dealer.\n",
        "plt.bar(): This generates a bar plot to compare the average selling prices across dealers. The bars represent the average price for each dealer.\n",
        "plt.xticks(rotation=45): This rotates the x-axis labels (dealers) for better readability.\n",
        "Make sure to replace 'dealer' and 'selling_price' with the actual column names from your dataset. This code will output the average selling prices for each dealer and visualize them in a bar chart.\n",
        "\n",
        "To determine which car brand (company) has the highest variation in prices, you can calculate the standard deviation of selling prices for each brand. The higher the standard deviation, the greater the variation in prices for that brand.\n",
        "\n",
        "Hereâ€™s how you can do it in Python:\n",
        "\n",
        "Steps:\n",
        "Group the dataset by the car brand (company).\n",
        "Calculate the standard deviation of the selling prices for each brand.\n",
        "Identify the brand with the highest standard deviation (i.e., the highest variation in prices).\n",
        "Hereâ€™s the Python code to perform this analysis:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the column for car brand is 'brand' and the column for selling price is 'selling_price'\n",
        "# Calculate the standard deviation of the selling prices for each car brand\n",
        "price_variation_by_brand = df.groupby('brand')['selling_price'].std()\n",
        "\n",
        "# Find the brand with the highest variation (highest standard deviation)\n",
        "highest_variation_brand = price_variation_by_brand.idxmax()\n",
        "highest_variation_value = price_variation_by_brand.max()\n",
        "\n",
        "# Print the result\n",
        "print(f\"The car brand with the highest variation in prices is '{highest_variation_brand}' with a price variation of {highest_variation_value:.2f}.\")\n",
        "Explanation:\n",
        "df.groupby('brand')['selling_price'].std(): Groups the dataset by the brand column and calculates the standard deviation of the selling prices for each brand.\n",
        "idxmax(): Finds the brand with the highest standard deviation.\n",
        "max(): Returns the highest variation (standard deviation) value.\n",
        "Insights:\n",
        "Highest Standard Deviation: A higher standard deviation indicates that there is a greater variation in the selling prices of cars within that brand. This could mean that the brand offers a wide range of models, from economy cars to luxury models, which have different price points.\n",
        "Low Standard Deviation: A low standard deviation indicates that the selling prices of cars within the brand are more consistent, suggesting the brand might offer cars at a relatively uniform price range.\n",
        "If you want to visualize the variations across brands, you can plot the standard deviations in a bar chart:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the price variation (standard deviation) by brand\n",
        "plt.figure(figsize=(10, 6))\n",
        "price_variation_by_brand.plot(kind='bar', color='skyblue')\n",
        "plt.title('Price Variation (Standard Deviation) by Car Brand')\n",
        "plt.xlabel('Car Brand')\n",
        "plt.ylabel('Price Variation (Standard Deviation)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "To analyze the distribution of car prices for each transmission type and compare the interquartile ranges (IQRs), you can follow these steps:\n",
        "\n",
        "Visualize the distribution: Use box plots to show how car prices are distributed for each transmission type.\n",
        "Calculate the IQR: The interquartile range is the difference between the first quartile (Q1) and the third quartile (Q3), which helps in understanding the spread of the data.\n",
        "Here's the Python code to perform this analysis:\n",
        "\n",
        "Step 1: Box Plot for Distribution of Car Prices by Transmission Type\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the columns are 'transmission' for transmission type and 'selling_price' for car price\n",
        "# Create a box plot to visualize the distribution of car prices by transmission type\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df, x='transmission', y='selling_price', palette='coolwarm')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Distribution of Car Prices by Transmission Type')\n",
        "plt.xlabel('Transmission Type')\n",
        "plt.ylabel('Selling Price')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Box plot: A box plot provides a clear picture of the distribution of car prices for each transmission type. It shows the median, the IQR (between Q1 and Q3), and potential outliers.\n",
        "sns.boxplot(): The x axis is the transmission type, and the y axis is the selling_price. You can adjust the palette to choose different color schemes.\n",
        "Step 2: Calculate and Compare the Interquartile Range (IQR)\n",
        "python\n",
        "Copy code\n",
        "# Calculate the IQR for car prices based on transmission type\n",
        "iqr_by_transmission = df.groupby('transmission')['selling_price'].apply(lambda x: x.quantile(0.75) - x.quantile(0.25))\n",
        "\n",
        "# Print the IQR for each transmission type\n",
        "print(\"Interquartile Range (IQR) for car prices by transmission type:\")\n",
        "print(iqr_by_transmission)\n",
        "Explanation:\n",
        "df.groupby('transmission')['selling_price'].apply(lambda x: x.quantile(0.75) - x.quantile(0.25)): This groups the data by transmission type and then calculates the IQR (Q3 - Q1) for each transmission type.\n",
        "quantile(0.25) and quantile(0.75): These methods compute the first and third quartiles, respectively.\n",
        "Insights:\n",
        "Box Plot Interpretation:\n",
        "The box plot shows how the car prices are distributed for each transmission type.\n",
        "The \"box\" represents the IQR, while the \"whiskers\" show the range of values within 1.5 times the IQR from the quartiles.\n",
        "Outliers are shown as individual points outside the whiskers.\n",
        "IQR:\n",
        "The IQR provides a measure of how spread out the middle 50% of the data is for each transmission type.\n",
        "A larger IQR indicates more variability in prices, while a smaller IQR suggests that the prices are more consistent within that transmission type.\n",
        "\n",
        "To analyze the distribution of car prices across different regions, we can use box plots or violin plots, which provide insights into the spread of car prices for each region. We can also calculate summary statistics like the median, quartiles, and interquartile range (IQR) to better understand the distribution.\n",
        "\n",
        "Hereâ€™s the Python code to visualize the distribution of car prices across different regions:\n",
        "\n",
        "Step 1: Box Plot for Distribution of Car Prices by Region\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the columns are 'region' for the region and 'selling_price' for car price\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a box plot to visualize the distribution of car prices by region\n",
        "sns.boxplot(data=df, x='region', y='selling_price', palette='coolwarm')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Distribution of Car Prices by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Selling Price')\n",
        "plt.xticks(rotation=45)  # Rotate region labels if needed\n",
        "plt.show()\n",
        "Explanation:\n",
        "Box plot: A box plot displays the distribution of car prices for each region. It shows the median price, the interquartile range (IQR), and potential outliers.\n",
        "sns.boxplot(): The x axis represents the region, and the y axis represents the selling_price.\n",
        "plt.xticks(rotation=45): Rotates the x-axis labels (regions) to make them more readable if they are long.\n",
        "Step 2: Violin Plot (Optional)\n",
        "A violin plot combines aspects of a box plot and a kernel density plot, providing a better understanding of the distribution shape.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Create a violin plot to visualize the distribution of car prices by region\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(data=df, x='region', y='selling_price', palette='coolwarm')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Distribution of Car Prices by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Selling Price')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "Explanation:\n",
        "Violin plot: This plot shows the probability density of car prices within each region. It helps understand whether the price distribution is skewed or symmetric and whether there are multiple modes (peaks).\n",
        "sns.violinplot(): The x axis is the region, and the y axis is the selling_price. This plot is particularly useful when comparing the distributions of prices between regions.\n",
        "Step 3: Calculate Summary Statistics (Optional)\n",
        "To get more insights into the price distributions, you can calculate summary statistics like the median, IQR, and mean for each region.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Calculate summary statistics for selling prices by region\n",
        "price_stats_by_region = df.groupby('region')['selling_price'].describe()\n",
        "\n",
        "# Print the summary statistics\n",
        "print(\"Summary statistics for car prices by region:\")\n",
        "print(price_stats_by_region)\n",
        "Explanation:\n",
        "df.groupby('region')['selling_price'].describe(): This groups the data by the region and calculates summary statistics such as the count, mean, standard deviation, minimum, 25th percentile (Q1), median (50th percentile), 75th percentile (Q3), and maximum values for selling_price.\n",
        "Insights:\n",
        "Box plot and violin plot: Both visualizations will help you see how car prices are distributed across different regions. Look for outliers, skewness, and the overall spread of prices.\n",
        "\n",
        "To analyze the distribution of cars based on body styles, you can use a bar plot to visualize how many cars belong to each body style. Additionally, you can calculate the count of cars for each body style to understand the distribution.\n",
        "\n",
        "Here's the Python code to perform this analysis:\n",
        "\n",
        "Step 1: Bar Plot for Distribution of Cars by Body Style\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the column for body style is 'body_style'\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a bar plot to visualize the count of cars for each body style\n",
        "sns.countplot(data=df, x='body_style', palette='Set2')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Distribution of Cars by Body Style')\n",
        "plt.xlabel('Body Style')\n",
        "plt.ylabel('Count of Cars')\n",
        "plt.xticks(rotation=45)  # Rotate labels if needed\n",
        "plt.show()\n",
        "Explanation:\n",
        "Bar plot: A bar plot is ideal for visualizing the count of cars in each body style category.\n",
        "sns.countplot(): The x axis represents the body_style, and the y axis shows the count (frequency) of cars for each body style.\n",
        "plt.xticks(rotation=45): Rotates the x-axis labels (body styles) if they are long or overlap.\n",
        "Step 2: Calculate the Count of Cars for Each Body Style (Optional)\n",
        "To get a numerical distribution, you can also calculate the count of cars for each body style.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Calculate the count of cars for each body style\n",
        "car_count_by_body_style = df['body_style'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(\"Count of cars for each body style:\")\n",
        "print(car_count_by_body_style)\n",
        "Explanation:\n",
        "df['body_style'].value_counts(): This function returns the count of cars for each body style, sorted by frequency.\n",
        "Insights:\n",
        "Bar Plot: The bar plot will show you the relative frequency of different body styles in your dataset, helping you understand which body styles are more common or rare.\n",
        "Count: The value_counts() function will provide you with the exact number of cars for each body style, which can help with more detailed analysis.\n",
        "\n",
        "To analyze how the average selling price of cars varies by customer gender and annual income, we can follow these steps:\n",
        "\n",
        "Group the data: Group the dataset by customer gender and annual income ranges, and then calculate the average selling price for each group.\n",
        "Visualization: Visualize the results using a bar plot or scatter plot to better understand the relationship between gender, income, and selling price.\n",
        "Assumptions:\n",
        "The dataset contains columns for customer gender and annual income (e.g., gender, annual_income, and selling_price).\n",
        "For better analysis, you might want to categorize annual income into bins (e.g., low, medium, high).\n",
        "Hereâ€™s the Python code to perform this analysis:\n",
        "\n",
        "Step 1: Categorize Annual Income and Calculate Average Selling Price\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the columns are 'gender', 'annual_income', and 'selling_price'\n",
        "\n",
        "# Categorize annual income into bins (e.g., low, medium, high)\n",
        "income_bins = [0, 30000, 60000, 100000, float('inf')]  # Adjust these ranges based on your data\n",
        "income_labels = ['Low', 'Medium', 'High', 'Very High']\n",
        "\n",
        "df['income_category'] = pd.cut(df['annual_income'], bins=income_bins, labels=income_labels)\n",
        "\n",
        "# Calculate the average selling price for each combination of gender and income category\n",
        "avg_price_by_gender_income = df.groupby(['gender', 'income_category'])['selling_price'].mean().reset_index()\n",
        "\n",
        "# Print the results\n",
        "print(\"Average selling price by gender and income category:\")\n",
        "print(avg_price_by_gender_income)\n",
        "Explanation:\n",
        "pd.cut(): This function categorizes the annual income into bins. You can adjust the income_bins and income_labels based on the actual income range in your dataset.\n",
        "groupby(['gender', 'income_category'])['selling_price'].mean(): This groups the data by both gender and income category, then calculates the average selling price for each group.\n",
        "reset_index(): This makes the result more readable and converts the group labels into columns.\n",
        "Step 2: Visualize the Average Selling Price by Gender and Income Category\n",
        "You can use a bar plot to visualize how the average selling price varies across different gender and income categories.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Create a bar plot to visualize the average selling price by gender and income category\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=avg_price_by_gender_income, x='income_category', y='selling_price', hue='gender', palette='Set1')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Average Selling Price by Gender and Income Category')\n",
        "plt.xlabel('Income Category')\n",
        "plt.ylabel('Average Selling Price')\n",
        "plt.show()\n",
        "Explanation:\n",
        "sns.barplot(): This creates a bar plot with income_category on the x-axis, the average selling_price on the y-axis, and colors representing different genders (hue='gender').\n",
        "palette='Set1': Specifies the color palette for better differentiation between genders.\n",
        "Insights:\n",
        "Bar Plot Interpretation: The plot will show how the average selling price differs across income categories and between genders. You can look for trends such as whether customers with higher income tend to buy more expensive cars or if there's a noticeable difference between male and female customers.\n",
        "Income vs. Selling Price: You may observe that higher-income groups tend to have higher average selling prices, but this can also vary by gender.\n",
        "\n",
        "To analyze the distribution of car prices by region and understand how the number of cars sold varies by region, we can break this down into two tasks:\n",
        "\n",
        "Distribution of Car Prices by Region: This can be visualized using box plots or violin plots to see the spread of car prices within each region.\n",
        "Number of Cars Sold by Region: We can visualize this using a bar plot to show how many cars were sold in each region.\n",
        "Step 1: Distribution of Car Prices by Region (Box Plot)\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the columns are 'region' for the region and 'selling_price' for car price\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create a box plot to visualize the distribution of car prices by region\n",
        "sns.boxplot(data=df, x='region', y='selling_price', palette='coolwarm')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Distribution of Car Prices by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Selling Price')\n",
        "plt.xticks(rotation=45)  # Rotate labels if needed\n",
        "plt.show()\n",
        "Explanation:\n",
        "Box Plot: This will show the median car price, the interquartile range (IQR), and any potential outliers in each region. The box represents the IQR, while the whiskers show the range of values within 1.5 times the IQR from the quartiles.\n",
        "sns.boxplot(): The x axis is the region, and the y axis is the selling_price.\n",
        "Step 2: Number of Cars Sold by Region (Bar Plot)\n",
        "python\n",
        "Copy code\n",
        "# Calculate the count of cars sold by region\n",
        "cars_sold_by_region = df['region'].value_counts()\n",
        "\n",
        "# Create a bar plot to visualize the number of cars sold by region\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=cars_sold_by_region.index, y=cars_sold_by_region.values, palette='Set2')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Number of Cars Sold by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Number of Cars Sold')\n",
        "plt.xticks(rotation=45)  # Rotate region labels for better readability\n",
        "plt.show()\n",
        "Explanation:\n",
        "Bar Plot: This will show the number of cars sold in each region. The length of each bar represents the count of cars sold in that particular region.\n",
        "sns.barplot(): The x axis represents the regions, and the y axis represents the count of cars sold.\n",
        "Step 3: (Optional) Summary Statistics by Region\n",
        "If you'd like to see more detailed statistics about the car prices in each region (e.g., mean, median, standard deviation), you can use the groupby() function:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Calculate summary statistics for selling prices by region\n",
        "price_stats_by_region = df.groupby('region')['selling_price'].describe()\n",
        "\n",
        "# Print the summary statistics\n",
        "print(\"Summary statistics for car prices by region:\")\n",
        "print(price_stats_by_region)\n",
        "Explanation:\n",
        "df.groupby('region')['selling_price'].describe(): This calculates the count, mean, standard deviation, min, 25th percentile, median (50th percentile), 75th percentile, and max for the selling prices in each region.\n",
        "Insights:\n",
        "Box Plot: The box plot will show how car prices are distributed across different regions. You can observe whether some regions have higher median prices or more spread-out prices (i.e., larger interquartile ranges).\n",
        "Bar Plot: The bar plot will give you a clear comparison of how many cars were sold in each region. Regions with higher sales may be more populated or have a higher demand for cars.\n",
        "\n",
        "To analyze how the average car price differs between cars with different engine sizes, you can follow these steps:\n",
        "\n",
        "Group the data: Group the dataset by engine_size and calculate the average car price for each engine size category.\n",
        "Visualization: Visualize the differences using a bar plot or scatter plot, which will help identify any trends or patterns.\n",
        "Assumptions:\n",
        "The dataset contains columns for engine_size (representing the car's engine size) and selling_price (representing the car's price).\n",
        "Hereâ€™s the Python code to perform this analysis:\n",
        "\n",
        "Step 1: Calculate Average Car Price by Engine Size\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Assuming the columns are 'engine_size' and 'selling_price'\n",
        "# Group by engine size and calculate the average selling price\n",
        "avg_price_by_engine = df.groupby('engine_size')['selling_price'].mean().reset_index()\n",
        "\n",
        "# Print the result\n",
        "print(\"Average car price by engine size:\")\n",
        "print(avg_price_by_engine)\n",
        "Explanation:\n",
        "df.groupby('engine_size')['selling_price'].mean(): This groups the data by engine_size and calculates the average selling_price for each engine size category.\n",
        "reset_index(): This ensures that the result is presented in a more readable format by converting the grouped labels into columns.\n",
        "Step 2: Visualize the Average Car Price by Engine Size (Bar Plot)\n",
        "python\n",
        "Copy code\n",
        "# Create a bar plot to visualize the average car price by engine size\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=avg_price_by_engine, x='engine_size', y='selling_price', palette='viridis')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Average Car Price by Engine Size')\n",
        "plt.xlabel('Engine Size')\n",
        "plt.ylabel('Average Selling Price')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
        "plt.show()\n",
        "Explanation:\n",
        "Bar Plot: This will help you visualize how the average price changes as the engine size varies. The x axis represents different engine sizes, and the y axis represents the average selling price for each engine size.\n",
        "sns.barplot(): It creates the bar plot, with engine_size on the x-axis and the average selling_price on the y-axis.\n",
        "Step 3: (Optional) Correlation Between Engine Size and Car Price\n",
        "You can also investigate if there is any correlation between engine_size and selling_price by calculating the correlation coefficient:\n",
        "\n",
        "python\n",
        "# Calculate the correlation between engine size and selling price\n",
        "correlation = df[['engine_size', 'selling_price']].corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(\"Correlation between engine size and selling price:\")\n",
        "print(correlation)\n",
        "Explanation:\n",
        "df[['engine_size', 'selling_price']].corr(): This calculates the correlation matrix between engine_size and selling_price. A positive correlation would indicate that larger engine sizes tend to have higher prices.\n",
        "Insights:\n",
        "Bar Plot Interpretation: The bar plot will show you the average price for different engine sizes. You might notice that cars with larger engine sizes tend to have higher average prices, but the extent of this relationship can be visualized.\n",
        "Correlation: If the correlation between engine_size and selling_price is strong, it would suggest that engine size is an important factor in determining car prices.\n",
        "\n",
        "To analyze how car prices vary based on the customerâ€™s annual income bracket, we can perform the following steps:\n",
        "\n",
        "Categorize Annual Income: Divide the annual income into meaningful brackets (e.g., low, medium, high, very high).\n",
        "Group and Calculate Average Prices: Group the data by income brackets and calculate the average car prices for each bracket.\n",
        "Visualization: Use a bar plot to visualize the relationship between income brackets and car prices.\n",
        "Step 1: Categorize Annual Income\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Categorize annual income into brackets\n",
        "income_bins = [0, 30000, 60000, 100000, float('inf')]  # Define income ranges\n",
        "income_labels = ['Low', 'Medium', 'High', 'Very High']  # Define income labels\n",
        "\n",
        "# Create a new column for income brackets\n",
        "df['income_bracket'] = pd.cut(df['annual_income'], bins=income_bins, labels=income_labels)\n",
        "\n",
        "# Check the distribution of income brackets\n",
        "print(df['income_bracket'].value_counts())\n",
        "Step 2: Calculate Average Car Prices by Income Bracket\n",
        "python\n",
        "# Group by income bracket and calculate the average car price\n",
        "avg_price_by_income = df.groupby('income_bracket')['selling_price'].mean().reset_index()\n",
        "\n",
        "# Print the results\n",
        "print(\"Average car prices by income bracket:\")\n",
        "print(avg_price_by_income)\n",
        "Step 3: Visualize the Relationship\n",
        "python\n",
        "# Create a bar plot to visualize the average car price by income bracket\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=avg_price_by_income, x='income_bracket', y='selling_price', palette='coolwarm')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Average Car Prices by Annual Income Bracket')\n",
        "plt.xlabel('Income Bracket')\n",
        "plt.ylabel('Average Selling Price')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Income Brackets:\n",
        "\n",
        "pd.cut(): This function creates the income_bracket column, categorizing the annual_income into predefined ranges.\n",
        "Adjust the ranges (income_bins) and labels (income_labels) based on the datasetâ€™s actual income distribution.\n",
        "Average Prices:\n",
        "\n",
        "The groupby() function calculates the average selling_price for each income bracket.\n",
        "Visualization:\n",
        "\n",
        "Bar Plot: The x axis shows income brackets, and the y axis displays the average selling price for each bracket.\n",
        "Insights:\n",
        "Average Prices: The bar plot shows how car prices vary across income brackets. Typically, higher income brackets may show higher average car prices, but there might be overlaps or exceptions.\n",
        "Distribution Check: It's important to review the distribution of income brackets (e.g., using value_counts()) to ensure thereâ€™s sufficient data in each category.\n",
        "\n",
        "10. What are the top 5 car models with the highest number of sales, and how does their price distribution look?\n",
        "To analyze the top 5 car models with the highest number of sales and examine their price distribution, follow these steps:\n",
        "\n",
        "Steps:\n",
        "Identify Top 5 Car Models: Group the data by car_model and count the number of cars sold for each model. Select the top 5 models with the highest sales.\n",
        "Visualize Price Distribution: Use a box plot to visualize the price distribution for these top 5 models.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Step 1: Identify the top 5 car models with the highest sales\n",
        "top_models = df['car_model'].value_counts().head(5).index\n",
        "top_model_counts = df['car_model'].value_counts().head(5)\n",
        "\n",
        "# Print the top 5 car models and their sales counts\n",
        "print(\"Top 5 car models with the highest number of sales:\")\n",
        "print(top_model_counts)\n",
        "\n",
        "# Step 2: Filter the dataset for the top 5 car models\n",
        "df_top_models = df[df['car_model'].isin(top_models)]\n",
        "\n",
        "# Step 3: Visualize the price distribution for the top 5 car models\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df_top_models, x='car_model', y='selling_price', palette='Set2')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Price Distribution for Top 5 Car Models')\n",
        "plt.xlabel('Car Model')\n",
        "plt.ylabel('Selling Price')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
        "plt.show()\n",
        "Explanation:\n",
        "Top 5 Models:\n",
        "\n",
        "value_counts(): Counts the number of occurrences (sales) for each car_model.\n",
        "head(5): Selects the top 5 car models with the highest sales.\n",
        "Filter Data:\n",
        "\n",
        "df[df['car_model'].isin(top_models)]: Filters the dataset to include only the rows corresponding to the top 5 car models.\n",
        "Box Plot:\n",
        "\n",
        "Box Plot Visualization: Shows the distribution of selling_price for each of the top 5 car models. The box represents the interquartile range (IQR), and whiskers show the range of data within 1.5 times the IQR. Outliers are plotted individually.\n",
        "Insights:\n",
        "Sales Count: The output of value_counts() will show the exact number of cars sold for each of the top 5 car models.\n",
        "Price Distribution: The box plot will allow you to compare the price ranges, medians, and spread of selling prices for the top 5 car models.\n",
        "Trends: Models with tighter box plots have more consistent pricing, while those with a larger spread may cater to a broader audience or include both basic and premium variants.\n",
        "\n",
        "Steps:\n",
        "Group Data: Group the data by color and engine_size, and calculate the average car price for each combination.\n",
        "Visualize Relationship: Use a line plot or scatter plot to observe trends of car prices with engine sizes for different colors.\n",
        "Identify Colors with Highest Variation: Calculate the price variation (e.g., standard deviation) for each color and identify the top colors with the highest variation.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Step 1: Group data by color and engine size and calculate average selling price\n",
        "avg_price_by_color_engine = df.groupby(['color', 'engine_size'])['selling_price'].mean().reset_index()\n",
        "\n",
        "# Print the grouped data\n",
        "print(\"Average car prices by color and engine size:\")\n",
        "print(avg_price_by_color_engine)\n",
        "\n",
        "# Step 2: Visualize the relationship using a line plot\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(data=avg_price_by_color_engine, x='engine_size', y='selling_price', hue='color', palette='tab10')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Car Price Variation with Engine Size Across Different Colors')\n",
        "plt.xlabel('Engine Size')\n",
        "plt.ylabel('Average Selling Price')\n",
        "plt.legend(title='Color', bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Calculate price variation for each color\n",
        "price_variation_by_color = df.groupby('color')['selling_price'].std().reset_index()\n",
        "price_variation_by_color.columns = ['color', 'price_std']\n",
        "\n",
        "# Sort by highest variation\n",
        "top_price_variation_colors = price_variation_by_color.sort_values(by='price_std', ascending=False)\n",
        "\n",
        "# Print the colors with the highest price variation\n",
        "print(\"Colors with the highest price variation:\")\n",
        "print(top_price_variation_colors)\n",
        "\n",
        "# Visualize price variation by color\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=top_price_variation_colors, x='color', y='price_std', palette='coolwarm')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Price Variation by Color')\n",
        "plt.xlabel('Color')\n",
        "plt.ylabel('Standard Deviation of Price')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.show()\n",
        "Explanation:\n",
        "Grouping:\n",
        "\n",
        "groupby(['color', 'engine_size'])['selling_price'].mean(): Calculates the average selling price for each color-engine size combination.\n",
        "Line Plot:\n",
        "\n",
        "sns.lineplot(): Visualizes how the average car price varies with engine size across different car colors.\n",
        "Hue: Differentiates the lines based on car colors.\n",
        "Price Variation:\n",
        "\n",
        "Standard Deviation: Measures the spread of car prices within each color group.\n",
        "sort_values(): Ranks the colors based on their price variation.\n",
        "Bar Plot:\n",
        "\n",
        "Visualizes the standard deviation of prices for each color to identify which colors exhibit the highest variation.\n",
        "\n",
        "Steps:\n",
        "Convert Sale Date to Datetime: Ensure the date_of_sale column is in a proper datetime format.\n",
        "Extract Month and Year: Extract the month and year from the date_of_sale column to analyze trends by month.\n",
        "Group Sales Data: Group the data by month to aggregate the number of sales.\n",
        "Visualize Seasonal Trends: Use a line plot to observe trends in car sales across months.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Step 1: Convert 'date_of_sale' to datetime\n",
        "df['date_of_sale'] = pd.to_datetime(df['date_of_sale'])\n",
        "\n",
        "# Step 2: Extract month and year\n",
        "df['sale_month'] = df['date_of_sale'].dt.month\n",
        "df['sale_year'] = df['date_of_sale'].dt.year\n",
        "\n",
        "# Step 3: Group by month to find the total number of sales\n",
        "monthly_sales = df.groupby('sale_month')['date_of_sale'].count().reset_index()\n",
        "monthly_sales.columns = ['month', 'sales_count']\n",
        "\n",
        "# Step 4: Visualize the seasonal trend\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=monthly_sales, x='month', y='sales_count', marker='o', palette='coolwarm')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Seasonal Trend in Car Sales')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Cars Sold')\n",
        "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Yearly Monthly Analysis (Optional)\n",
        "monthly_sales_by_year = df.groupby(['sale_year', 'sale_month'])['date_of_sale'].count().reset_index()\n",
        "monthly_sales_by_year.columns = ['year', 'month', 'sales_count']\n",
        "\n",
        "# Line plot showing monthly trends across years\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(data=monthly_sales_by_year, x='month', y='sales_count', hue='year', marker='o', palette='tab10')\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Monthly Car Sales Trends Over the Years')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Cars Sold')\n",
        "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.legend(title='Year')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "Explanation:\n",
        "Datetime Conversion:\n",
        "\n",
        "pd.to_datetime() ensures that the date_of_sale column is in a datetime format for easier manipulation.\n",
        "Extracting Month:\n",
        "\n",
        ".dt.month extracts the month from the date, allowing for grouping by month to identify seasonal patterns.\n",
        "Monthly Sales Aggregation:\n",
        "\n",
        "groupby('sale_month')['date_of_sale'].count() aggregates the total number of sales for each month.\n",
        "Line Plot:\n",
        "\n",
        "The line plot highlights peaks and troughs in sales activity across months, making seasonal trends easily identifiable.\n",
        "Yearly Trends (Optional):\n",
        "\n",
        "The second visualization shows how monthly sales trends vary across different years, providing a deeper understanding of year-over-year seasonality.\n",
        "\n",
        "Steps:\n",
        "Group Data: Group the data by body_style and transmission_type to observe price statistics for each combination.\n",
        "Visualize Distribution: Use a box plot to compare the distribution of car prices across the different combinations of body style and transmission type.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Step 1: Check for unique body styles and transmission types\n",
        "print(\"Unique Body Styles:\", df['body_style'].unique())\n",
        "print(\"Unique Transmission Types:\", df['transmission_type'].unique())\n",
        "\n",
        "# Step 2: Group data by body style and transmission type\n",
        "grouped_data = df.groupby(['body_style', 'transmission_type'])['selling_price'].describe()\n",
        "print(\"Summary statistics for price by body style and transmission type:\")\n",
        "print(grouped_data)\n",
        "\n",
        "# Step 3: Visualize the distribution of car prices using a boxplot\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(\n",
        "    data=df,\n",
        "    x='body_style',\n",
        "    y='selling_price',\n",
        "    hue='transmission_type',\n",
        "    palette='coolwarm'\n",
        ")\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Car Price Distribution by Body Style and Transmission Type')\n",
        "plt.xlabel('Body Style')\n",
        "plt.ylabel('Selling Price')\n",
        "plt.legend(title='Transmission Type', bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
        "plt.show()\n",
        "Explanation:\n",
        "Grouping Data:\n",
        "\n",
        "groupby(['body_style', 'transmission_type'])['selling_price'].describe() provides descriptive statistics (mean, median, min, max, etc.) for car prices within each combination of body_style and transmission_type.\n",
        "Box Plot:\n",
        "\n",
        "sns.boxplot() visualizes the distribution of car prices for each body style, further split by transmission type (hue parameter).\n",
        "The box plot shows the interquartile range (IQR), median, and outliers for each combination.\n",
        "Customizations:\n",
        "\n",
        "Rotate x-axis labels for better readability if the body_style names are long.\n",
        "Adjust the legend to fit neatly within the plot area.\n",
        "\n",
        "Steps:\n",
        "Select Relevant Features: Focus on selling_price, engine_size, and annual_income.\n",
        "Compute Correlation Matrix: Use the Pearson correlation coefficient to measure relationships between these numerical variables.\n",
        "Visualize Correlations: Use a heatmap to visualize the correlation matrix and a pair plot to examine feature interactions.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Step 1: Select relevant features\n",
        "selected_columns = ['selling_price', 'engine_size', 'annual_income']\n",
        "correlation_data = df[selected_columns]\n",
        "\n",
        "# Step 2: Compute the correlation matrix\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Step 3: Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Correlation Matrix of Car Price, Engine Size, and Annual Income')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Visualize feature interactions using a pair plot\n",
        "sns.pairplot(correlation_data, diag_kind='kde', markers='o', palette='Set2')\n",
        "\n",
        "# Set plot titles\n",
        "plt.suptitle('Feature Interactions Between Car Price, Engine Size, and Annual Income', y=1.02)\n",
        "plt.show()\n",
        "Explanation:\n",
        "Correlation Matrix:\n",
        "\n",
        "df.corr() computes the Pearson correlation coefficient between numerical columns.\n",
        "Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear correlation.\n",
        "Heatmap:\n",
        "\n",
        "sns.heatmap() visualizes the correlation matrix.\n",
        "Annotated values help identify strong or weak correlations at a glance.\n",
        "Pair Plot:\n",
        "\n",
        "sns.pairplot() creates scatter plots for feature pairs and KDE (Kernel Density Estimate) plots on the diagonal.\n",
        "This helps visualize the distribution of individual features and how they interact pairwise.\n",
        "\n",
        "Steps:\n",
        "Group Data: Group the data by car_model and engine_type and calculate the average selling price for each group.\n",
        "Visualize the Data: Use a bar plot or heatmap to display the average prices across combinations of car models and engine types.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'car_sale.csv' with your actual file path)\n",
        "df = pd.read_csv('car_sale.csv')\n",
        "\n",
        "# Step 1: Group data by car model and engine type to calculate average price\n",
        "average_price = df.groupby(['car_model', 'engine_type'])['selling_price'].mean().reset_index()\n",
        "\n",
        "# Print the grouped data\n",
        "print(\"Average car prices by model and engine type:\")\n",
        "print(average_price)\n",
        "\n",
        "# Step 2: Pivot the data for better visualization in a heatmap\n",
        "pivot_table = average_price.pivot(index='car_model', columns='engine_type', values='selling_price')\n",
        "\n",
        "# Step 3: Visualize using a heatmap\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, cbar_kws={'label': 'Average Price'})\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Average Car Price by Model and Engine Type')\n",
        "plt.xlabel('Engine Type')\n",
        "plt.ylabel('Car Model')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Bar plot (Optional)\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.barplot(\n",
        "    data=average_price,\n",
        "    x='car_model',\n",
        "    y='selling_price',\n",
        "    hue='engine_type',\n",
        "    palette='coolwarm'\n",
        ")\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Average Car Price by Model and Engine Type')\n",
        "plt.xlabel('Car Model')\n",
        "plt.ylabel('Average Selling Price')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.legend(title='Engine Type')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Grouping Data:\n",
        "\n",
        "groupby(['car_model', 'engine_type'])['selling_price'].mean() calculates the average selling price for each combination of car model and engine type.\n",
        "Heatmap Visualization:\n",
        "\n",
        "pivot() restructures the data for a heatmap, with car_model as rows and engine_type as columns.\n",
        "sns.heatmap() visually highlights the variation in average prices across car models and engine types.\n",
        "Bar Plot (Optional):\n",
        "\n",
        "sns.barplot() displays the average prices as bars, grouped by engine type for each car model.\n",
        "Useful for seeing precise differences in price across engine types.\n",
        "\n",
        "\n",
        "EDA - 3\n",
        "Amazon Sales Data\n",
        "1. What is the average rating for each product category?\n",
        "\n",
        "Steps:\n",
        "Group Data: Group the data by the product_category column and calculate the mean of the rating column.\n",
        "Handle Missing Ratings: Ensure there are no missing values in the rating column before calculation.\n",
        "Visualize the Results: Use a bar plot to display the average ratings for each product category.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Step 1: Handle missing ratings (if any)\n",
        "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')  # Convert to numeric, coerce errors\n",
        "df = df.dropna(subset=['rating'])  # Drop rows where rating is NaN\n",
        "\n",
        "# Step 2: Group by product category and calculate the average rating\n",
        "average_ratings = df.groupby('product_category')['rating'].mean().reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "average_ratings.columns = ['Product Category', 'Average Rating']\n",
        "\n",
        "# Print the results\n",
        "print(\"Average ratings for each product category:\")\n",
        "print(average_ratings)\n",
        "\n",
        "# Step 3: Visualize the average ratings using a bar plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    data=average_ratings,\n",
        "    x='Average Rating',\n",
        "    y='Product Category',\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Average Rating for Each Product Category')\n",
        "plt.xlabel('Average Rating')\n",
        "plt.ylabel('Product Category')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Missing Value Handling:\n",
        "\n",
        "pd.to_numeric(..., errors='coerce') ensures non-numeric values in the rating column are converted to NaN.\n",
        "dropna() removes rows where the rating is missing.\n",
        "Grouping and Calculation:\n",
        "\n",
        "groupby('product_category')['rating'].mean() calculates the mean rating for each product category.\n",
        "Visualization:\n",
        "\n",
        "sns.barplot() creates a horizontal bar plot with product categories on the y-axis and average ratings on the x-axis.\n",
        "\n",
        "Steps:\n",
        "Group Data: Group the data by product_category and identify the product with the highest rating_count in each category.\n",
        "Sort Data: For each product_category, find the product with the maximum rating_count.\n",
        "Display Results: Create a summary of the top products with the highest rating_count in each category.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Ensure `rating_count` is numeric\n",
        "df['rating_count'] = pd.to_numeric(df['rating_count'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values in 'product_category' or 'rating_count'\n",
        "df = df.dropna(subset=['product_category', 'rating_count'])\n",
        "\n",
        "# Step 1: Find the top products by rating_count for each category\n",
        "top_rating_count_products = df.loc[df.groupby('product_category')['rating_count'].idxmax()]\n",
        "\n",
        "# Step 2: Select relevant columns for the result\n",
        "result = top_rating_count_products[['product_category', 'product_name', 'rating_count']]\n",
        "\n",
        "# Print the results\n",
        "print(\"Top products by rating count for each category:\")\n",
        "print(result)\n",
        "\n",
        "# Optional: Save to a CSV file\n",
        "result.to_csv('top_rating_count_products_by_category.csv', index=False)\n",
        "Explanation:\n",
        "Data Cleaning:\n",
        "\n",
        "Ensure that the rating_count column is numeric using pd.to_numeric().\n",
        "Drop rows with missing values in the relevant columns.\n",
        "Group and Aggregate:\n",
        "\n",
        "groupby('product_category')['rating_count'].idxmax() finds the index of the product with the maximum rating_count for each category.\n",
        "Subset Columns:\n",
        "\n",
        "Select the relevant columns (product_category, product_name, rating_count) for clarity.\n",
        "Output:\n",
        "\n",
        "Print the results or save them to a CSV file for further analysis.\n",
        "\n",
        "Steps:\n",
        "Calculate Discounted Prices: If not already available, calculate discounted prices using actual prices and discount percentages.\n",
        "Visualize Distributions: Use histograms or KDE plots to compare the distributions of discounted and actual prices.\n",
        "Overlay for Comparison: Overlay both distributions in a single plot for better comparison.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Ensure required columns are numeric\n",
        "df['actual_price'] = pd.to_numeric(df['actual_price'], errors='coerce')\n",
        "df['discounted_price'] = pd.to_numeric(df['discounted_price'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing prices\n",
        "df = df.dropna(subset=['actual_price', 'discounted_price'])\n",
        "\n",
        "# Step 1: Visualize the distribution of actual prices and discounted prices\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Histogram for actual prices\n",
        "sns.kdeplot(df['actual_price'], label='Actual Price', color='blue', fill=True, alpha=0.3)\n",
        "\n",
        "# Histogram for discounted prices\n",
        "sns.kdeplot(df['discounted_price'], label='Discounted Price', color='green', fill=True, alpha=0.3)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Distribution of Discounted Prices vs. Actual Prices')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "Explanation:\n",
        "Data Cleaning:\n",
        "\n",
        "Ensure the actual_price and discounted_price columns are numeric using pd.to_numeric().\n",
        "Remove rows with missing values in these columns.\n",
        "Visualization:\n",
        "\n",
        "sns.kdeplot() creates smooth density plots for actual_price and discounted_price.\n",
        "Use different colors and labels for the two distributions.\n",
        "Comparison:\n",
        "\n",
        "Overlay the distributions to see how discounted prices differ from actual prices.\n",
        "\n",
        "Steps:\n",
        "Calculate Discount Percentage: If not already available, calculate the discount percentage using the formula:\n",
        "DiscountÂ Percentage\n",
        "=\n",
        "(\n",
        "ActualÂ Price\n",
        "âˆ’\n",
        "DiscountedÂ Price\n",
        "ActualÂ Price\n",
        ")\n",
        "Ã—\n",
        "100\n",
        "DiscountÂ Percentage=(\n",
        "ActualÂ Price\n",
        "ActualÂ Priceâˆ’DiscountedÂ Price\n",
        "â€‹\n",
        " )Ã—100\n",
        "Group by Category: Group the data by product_category and calculate the average discount percentage for each category.\n",
        "Visualize Results: Use a bar plot to show the average discount percentage for each category.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Ensure the required columns are numeric\n",
        "df['actual_price'] = pd.to_numeric(df['actual_price'], errors='coerce')\n",
        "df['discounted_price'] = pd.to_numeric(df['discounted_price'], errors='coerce')\n",
        "\n",
        "# Step 1: Calculate the discount percentage\n",
        "df['discount_percentage'] = ((df['actual_price'] - df['discounted_price']) / df['actual_price']) * 100\n",
        "\n",
        "# Drop rows with missing discount values\n",
        "df = df.dropna(subset=['discount_percentage'])\n",
        "\n",
        "# Step 2: Group by product category and calculate the average discount percentage\n",
        "average_discount = df.groupby('product_category')['discount_percentage'].mean().reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "average_discount.columns = ['Product Category', 'Average Discount Percentage']\n",
        "\n",
        "# Step 3: Visualize the average discount percentage by category\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(\n",
        "    data=average_discount,\n",
        "    x='Average Discount Percentage',\n",
        "    y='Product Category',\n",
        "    palette='coolwarm'\n",
        ")\n",
        "\n",
        "# Set plot titles and labels\n",
        "plt.title('Average Discount Percentage Across Product Categories')\n",
        "plt.xlabel('Average Discount Percentage')\n",
        "plt.ylabel('Product Category')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Calculate Discount Percentage:\n",
        "\n",
        "(actual_price - discounted_price) / actual_price * 100 calculates the discount percentage for each product.\n",
        "Group and Calculate:\n",
        "\n",
        "groupby('product_category')['discount_percentage'].mean() computes the average discount percentage for each category.\n",
        "Visualization:\n",
        "\n",
        "sns.barplot() visualizes the average discount percentage across categories, with categories on the y-axis and average discount percentage on the x-axis.\n",
        "\n",
        "To identify the most popular product names, we typically look at a metric like the number of times each product appears in the dataset, assuming the dataset has a product_name column. This could be based on the number of sales, customer ratings, or frequency of listings.\n",
        "\n",
        "If the dataset contains the product_name and sales_count or purchase_count columns, we can find the most popular products by grouping the data by product_name and counting their occurrences or summing up their sales.\n",
        "\n",
        "Steps:\n",
        "Group by Product Name: Group the dataset by product_name and count the occurrences or sum the sales_count (or similar metrics) to find the most popular products.\n",
        "Sort by Popularity: Sort the products by the number of occurrences or sales count to identify the top products.\n",
        "Display Results: Show the most popular product names.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Ensure required columns are present\n",
        "# Assuming 'product_name' and 'sales_count' or 'purchase_count' exist\n",
        "df['sales_count'] = pd.to_numeric(df['sales_count'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing sales data\n",
        "df = df.dropna(subset=['sales_count'])\n",
        "\n",
        "# Step 1: Group by product name and sum sales count or occurrences\n",
        "product_popularity = df.groupby('product_name')['sales_count'].sum().reset_index()\n",
        "\n",
        "# Step 2: Sort by sales count in descending order to get the most popular products\n",
        "top_products = product_popularity.sort_values(by='sales_count', ascending=False).head(10)\n",
        "\n",
        "# Print the top 10 most popular products\n",
        "print(\"Top 10 Most Popular Products:\")\n",
        "print(top_products)\n",
        "\n",
        "# Optional: Save to a CSV file\n",
        "top_products.to_csv('top_popular_products.csv', index=False)\n",
        "Explanation:\n",
        "Grouping and Summing:\n",
        "\n",
        "groupby('product_name')['sales_count'].sum() groups the dataset by product_name and sums the sales or purchase counts to find the most popular products.\n",
        "Sorting:\n",
        "\n",
        "sort_values(by='sales_count', ascending=False) sorts the products in descending order of popularity.\n",
        "Output:\n",
        "\n",
        "The top 10 most popular products are printed or saved to a CSV file.\n",
        "\n",
        "To identify the most popular product keywords, we need to extract the most frequent terms or phrases from the product names or descriptions in the dataset. This analysis often involves:\n",
        "\n",
        "Tokenizing: Breaking down the product names (or descriptions) into individual words.\n",
        "Cleaning the Text: Removing stopwords (common, unimportant words like \"and\", \"the\", etc.) and possibly punctuation.\n",
        "Counting Frequency: Counting the frequency of each keyword (or term) across the dataset.\n",
        "Visualizing: Displaying the most popular keywords using a bar plot or word cloud.\n",
        "Steps:\n",
        "Extract Keywords: Use the product names or descriptions and split them into individual words (tokens).\n",
        "Clean Text: Remove stopwords and punctuation to focus on the important terms.\n",
        "Count Frequency: Calculate the frequency of each keyword across all product names.\n",
        "Visualize: Display the top keywords using a bar plot or word cloud.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Step 1: Extract the product names\n",
        "# Assuming 'product_name' column contains the product names\n",
        "product_names = df['product_name'].dropna()\n",
        "\n",
        "# Step 2: Clean and process the text (remove punctuation and stopwords)\n",
        "stop_words = set(stopwords.words('english'))  # Get English stopwords\n",
        "\n",
        "# Create a function to clean product names\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize, convert to lower case, and remove stopwords\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply the cleaning function to the product names\n",
        "cleaned_text = product_names.apply(clean_text)\n",
        "\n",
        "# Step 3: Use CountVectorizer to extract keywords\n",
        "vectorizer = CountVectorizer(max_features=20, ngram_range=(1, 2))  # Extract top 20 unigrams and bigrams\n",
        "X = vectorizer.fit_transform(cleaned_text)\n",
        "\n",
        "# Get the frequency of the terms\n",
        "keywords = vectorizer.get_feature_names_out()\n",
        "keyword_counts = X.sum(axis=0).A1  # Get sum of counts for each word\n",
        "\n",
        "# Step 4: Create a DataFrame for the top keywords\n",
        "keyword_df = pd.DataFrame({'Keyword': keywords, 'Count': keyword_counts})\n",
        "keyword_df = keyword_df.sort_values(by='Count', ascending=False)\n",
        "\n",
        "# Print the top 10 most popular keywords\n",
        "print(\"Top 10 Most Popular Product Keywords:\")\n",
        "print(keyword_df.head(10))\n",
        "\n",
        "# Optional: Create a Word Cloud for visualization\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(zip(keyword_df['Keyword'], keyword_df['Count'])))\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Plot bar chart for top 10 keywords\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(keyword_df['Keyword'].head(10), keyword_df['Count'].head(10), color='teal')\n",
        "plt.xlabel('Count')\n",
        "plt.title('Top 10 Most Popular Product Keywords')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the highest count at the top\n",
        "plt.show()\n",
        "Explanation:\n",
        "Text Cleaning:\n",
        "\n",
        "clean_text() function removes punctuation and stopwords (commonly used words that donâ€™t carry significant meaning, such as \"the\", \"is\", \"and\").\n",
        "Keyword Extraction:\n",
        "\n",
        "CountVectorizer extracts the top keywords (unigrams and bigrams) from the cleaned product names. max_features=20 limits the output to the top 20 keywords, and ngram_range=(1, 2) includes both single words and two-word combinations.\n",
        "Word Cloud:\n",
        "\n",
        "WordCloud() creates a visual representation of the most frequent keywords. Larger words appear more frequently in the dataset.\n",
        "Bar Plot:\n",
        "\n",
        "A bar plot shows the top 10 most frequent keywords based on their count.\n",
        "\n",
        "To identify the most popular product reviews, we can focus on highly rated reviews or reviews with the most mentions or likes (if available in your dataset). If the dataset has a column for review text, rating, or number of likes, we can analyze the reviews based on these metrics.\n",
        "\n",
        "If we assume the dataset contains the following columns:\n",
        "\n",
        "product_name: Name of the product.\n",
        "review_text: The text of the review.\n",
        "rating: Rating given by the customer (e.g., from 1 to 5 stars).\n",
        "likes_count or helpful_count: The number of likes or helpful votes a review has received.\n",
        "We can identify the most popular reviews based on:\n",
        "\n",
        "Highest Rating: Reviews with the highest ratings.\n",
        "Most Liked or Helpful: Reviews with the highest number of likes or helpful votes.\n",
        "Positive Sentiment: Reviews that express the most positive sentiment.\n",
        "Steps:\n",
        "Filter for Top Reviews: Filter the reviews that have the highest ratings or most likes/helpful votes.\n",
        "Analyze the Review Text: Extract the most common phrases or keywords from the highly rated reviews.\n",
        "Visualize Popular Reviews: Display the reviews that have the highest number of likes or ratings.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Ensure required columns are present\n",
        "# Assuming 'product_name', 'review_text', 'rating', and 'likes_count' columns exist\n",
        "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
        "df['likes_count'] = pd.to_numeric(df['likes_count'], errors='coerce')\n",
        "\n",
        "# Step 1: Filter for the top reviews based on rating or likes_count\n",
        "top_rated_reviews = df.sort_values(by='rating', ascending=False).head(10)  # Top 10 highest-rated reviews\n",
        "\n",
        "# Or alternatively, if you want the reviews with the most likes/helpful votes\n",
        "most_liked_reviews = df.sort_values(by='likes_count', ascending=False).head(10)  # Top 10 most liked reviews\n",
        "\n",
        "# Step 2: Display the most popular reviews (can be by rating or by likes)\n",
        "print(\"Top 10 Most Highly Rated Reviews:\")\n",
        "print(top_rated_reviews[['product_name', 'review_text', 'rating']])\n",
        "\n",
        "# Alternatively, display the most liked reviews\n",
        "print(\"Top 10 Most Liked Reviews:\")\n",
        "print(most_liked_reviews[['product_name', 'review_text', 'likes_count']])\n",
        "\n",
        "# Optional: Save the top reviews to a CSV file\n",
        "top_rated_reviews.to_csv('top_rated_reviews.csv', index=False)\n",
        "most_liked_reviews.to_csv('most_liked_reviews.csv', index=False)\n",
        "Explanation:\n",
        "Sort and Filter:\n",
        "\n",
        "We sort the dataset by the rating column to find the highest-rated reviews. Alternatively, sorting by likes_count finds reviews with the most likes or helpful votes.\n",
        "Display Top Reviews:\n",
        "\n",
        "We show the top 10 highest-rated reviews (top_rated_reviews) or the top 10 most liked reviews (most_liked_reviews).\n",
        "Output:\n",
        "\n",
        "Display the product name, review text, rating, and likes count for the most popular reviews.\n",
        "\n",
        "To analyze the correlation between discounted_price and rating, we can calculate the correlation coefficient (typically Pearson correlation) to measure the linear relationship between these two variables. A positive correlation would indicate that higher discounted prices tend to be associated with higher ratings, while a negative correlation would indicate the opposite.\n",
        "\n",
        "Steps:\n",
        "Clean the Data: Ensure that discounted_price and rating columns have valid numeric values and handle any missing values.\n",
        "Calculate the Correlation: Use the Pearson correlation coefficient to calculate the relationship between discounted_price and rating.\n",
        "Visualize the Relationship: Optionally, create a scatter plot to visually inspect how the variables relate.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Step 1: Clean data (ensure numeric columns and handle missing values)\n",
        "df['discounted_price'] = pd.to_numeric(df['discounted_price'], errors='coerce')\n",
        "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values in either column\n",
        "df_clean = df.dropna(subset=['discounted_price', 'rating'])\n",
        "\n",
        "# Step 2: Calculate the Pearson correlation coefficient\n",
        "correlation = df_clean[['discounted_price', 'rating']].corr().iloc[0, 1]\n",
        "\n",
        "# Print the correlation coefficient\n",
        "print(f\"Pearson correlation between discounted_price and rating: {correlation}\")\n",
        "\n",
        "# Step 3: Visualize the relationship using a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=df_clean, x='discounted_price', y='rating', color='teal')\n",
        "plt.title('Relationship between Discounted Price and Rating')\n",
        "plt.xlabel('Discounted Price')\n",
        "plt.ylabel('Rating')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Cleaning the Data:\n",
        "\n",
        "The pd.to_numeric() function ensures that discounted_price and rating are numeric values. If any values are non-numeric or missing, they are coerced into NaN, which we then drop using dropna().\n",
        "Calculating the Correlation:\n",
        "\n",
        "df_clean[['discounted_price', 'rating']].corr() calculates the Pearson correlation coefficient between the two variables.\n",
        "The Pearson correlation coefficient is a value between -1 and 1, where:\n",
        "1 indicates a perfect positive linear relationship.\n",
        "-1 indicates a perfect negative linear relationship.\n",
        "0 indicates no linear relationship.\n",
        "Visualization:\n",
        "\n",
        "A scatter plot helps visualize the relationship between discounted_price and rating. If thereâ€™s a pattern (e.g., points forming a line), the correlation coefficient will reflect that.\n",
        "\n",
        "To find the Top 5 categories based on the highest ratings, we need to:\n",
        "\n",
        "Group the data by category: Weâ€™ll group the dataset by the product category (assuming such a column exists).\n",
        "Calculate the average rating for each category: We will compute the mean rating for each category.\n",
        "Sort the categories by average rating: Sort the categories in descending order based on their average rating.\n",
        "Select the Top 5 categories: Extract the top 5 categories with the highest average ratings.\n",
        "Python Code:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Step 1: Clean data (ensure numeric rating column and handle missing values)\n",
        "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
        "\n",
        "# Step 2: Group by category and calculate the mean rating for each category\n",
        "# Assuming 'category' is the column representing product categories\n",
        "category_ratings = df.groupby('category')['rating'].mean().reset_index()\n",
        "\n",
        "# Step 3: Sort the categories by average rating in descending order\n",
        "top_categories = category_ratings.sort_values(by='rating', ascending=False).head(5)\n",
        "\n",
        "# Step 4: Display the top 5 categories with the highest ratings\n",
        "print(\"Top 5 Categories Based on Highest Ratings:\")\n",
        "print(top_categories)\n",
        "Explanation:\n",
        "Data Cleaning: Ensure that the rating column contains numeric values (using pd.to_numeric()) and handle any missing or invalid values.\n",
        "Group and Calculate Average Rating: We use groupby('category')['rating'].mean() to calculate the average rating for each product category.\n",
        "Sort and Select Top 5: The categories are sorted by average rating in descending order, and the top 5 are selected using .head(5).\n",
        "\n",
        "\n",
        "Key Areas for Analysis:\n",
        "Identify products or categories with low ratings.\n",
        "Examine products with high discounts and low ratings.\n",
        "Check for missing data.\n",
        "Analyze products with high prices and low ratings.\n",
        "Python Code for Identifying Areas for Improvement:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'amazon_sales_data.csv' with your actual file path)\n",
        "df = pd.read_csv('amazon_sales_data.csv')\n",
        "\n",
        "# Step 1: Clean data (ensure numeric columns and handle missing values)\n",
        "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
        "df['discounted_price'] = pd.to_numeric(df['discounted_price'], errors='coerce')\n",
        "\n",
        "# Step 2: Identify products or categories with low ratings (e.g., ratings <= 2)\n",
        "low_rated_products = df[df['rating'] <= 2]\n",
        "\n",
        "# Step 3: Analyze products with high discounts but low ratings (e.g., high discount price and rating <= 2)\n",
        "high_discount_low_rating = df[(df['discounted_price'] > df['discounted_price'].median()) & (df['rating'] <= 2)]\n",
        "\n",
        "# Step 4: Identify missing data in key columns (e.g., rating, discounted_price, category)\n",
        "missing_data = df.isnull().sum()\n",
        "\n",
        "# Step 5: Analyze products with high prices and low ratings (e.g., products with high discounted price but low ratings)\n",
        "high_price_low_rating = df[(df['discounted_price'] > df['discounted_price'].quantile(0.75)) & (df['rating'] <= 2)]\n",
        "\n",
        "# Step 6: Output insights\n",
        "print(\"Low Rated Products (Rating <= 2):\")\n",
        "print(low_rated_products[['product_name', 'category', 'rating']].head(10))\n",
        "\n",
        "print(\"\\nHigh Discounted Products with Low Ratings:\")\n",
        "print(high_discount_low_rating[['product_name', 'discounted_price', 'rating']].head(10))\n",
        "\n",
        "print(\"\\nMissing Data Overview:\")\n",
        "print(missing_data)\n",
        "\n",
        "print(\"\\nHigh Price Products with Low Ratings:\")\n",
        "print(high_price_low_rating[['product_name', 'discounted_price', 'rating']].head(10))\n",
        "\n",
        "Explanation of the Code:\n",
        "Low Rating Products: We filter the products where the rating is less than or equal to 2 to identify items that are underperforming in customer satisfaction.\n",
        "High Discount and Low Rating: This checks for products with discounted prices greater than the median discounted price but having ratings of 2 or less. These products may not be meeting customer expectations despite being discounted.\n",
        "Missing Data: We check for missing data in key columns such as rating, discounted_price, and category, as missing values can indicate incomplete product information.\n",
        "High Price and Low Rating: This focuses on products with the highest prices but low ratings, which could indicate that customers expect better value for money.\n",
        "\n",
        "EDA - 4\n",
        "1. Read the dataframe, check null value if present then do the needful, check duplicate row , if present then do\n",
        "the needful?\n",
        "To begin analyzing the Spotify Data: Popular Hip-hop Artists and Tracks, we can follow these steps to:\n",
        "\n",
        "Read the Data: Load the dataset into a DataFrame.\n",
        "Check for Null Values: Identify any missing values in the dataset.\n",
        "Handle Missing Data: Impute or drop rows/columns with missing values as needed.\n",
        "Check for Duplicates: Identify any duplicate rows.\n",
        "Remove Duplicates: Remove duplicate rows if necessary.\n",
        "Python Code to Perform These Steps:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 2: Check for null values\n",
        "print(\"Null Values in the Dataset:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Step 3: Handle missing data\n",
        "# Option 1: Drop rows with any missing values (if necessary)\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "# Option 2: Impute missing values (e.g., for 'Popularity' and 'Duration' columns)\n",
        "# df['Popularity'] = df['Popularity'].fillna(df['Popularity'].mean())\n",
        "# df['Duration (ms)'] = df['Duration (ms)'].fillna(df['Duration (ms)'].mean())\n",
        "\n",
        "# Step 4: Check for duplicate rows\n",
        "print(\"\\nDuplicate Rows in the Dataset:\")\n",
        "print(df.duplicated().sum())\n",
        "\n",
        "# Step 5: Remove duplicates if any\n",
        "df_cleaned = df_cleaned.drop_duplicates()\n",
        "\n",
        "# Check the cleaned data\n",
        "print(\"\\nCleaned Data Preview:\")\n",
        "print(df_cleaned.head())\n",
        "Explanation:\n",
        "Reading the Dataset: The dataset is loaded using pd.read_csv().\n",
        "Check for Null Values: The .isnull().sum() method helps identify any missing values in the dataset.\n",
        "If null values are found, you can either drop those rows (dropna()) or impute them using a central tendency (e.g., mean or median).\n",
        "Check for Duplicates: The .duplicated().sum() function counts the number of duplicate rows.\n",
        "If duplicates are present, the drop_duplicates() function removes them.\n",
        "Display Cleaned Data: After handling missing values and duplicates, the cleaned data is displayed using .head() to inspect the first few rows.\n",
        "\n",
        "To visualize the distribution of popularity among the tracks in the dataset, we can use a histogram. This will allow us to see how the popularity scores are spread across the tracks.\n",
        "\n",
        "Steps to Visualize the Distribution of Popularity:\n",
        "Extract the Popularity Column: Focus on the Popularity column.\n",
        "Plot a Histogram: Use a histogram to visualize the distribution.\n",
        "Python Code to Visualize Popularity Distribution:\n",
        "python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing popularity values)\n",
        "df_cleaned = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Step 2: Plot the distribution of popularity using a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_cleaned['Popularity'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Popularity Among Hip-hop Tracks')\n",
        "plt.xlabel('Popularity')\n",
        "plt.ylabel('Number of Tracks')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "Explanation:\n",
        "Drop Missing Popularity Values: We use dropna(subset=['Popularity']) to remove rows where the Popularity value is missing.\n",
        "Plot Histogram:\n",
        "plt.hist() generates the histogram with the Popularity values.\n",
        "bins=20 determines the number of bins (you can adjust this value based on the range of popularity values).\n",
        "color='skyblue' and edgecolor='black' are used for styling the histogram bars.\n",
        "Labels: The xlabel, ylabel, and title functions are used to add labels to the axes and title to the plot.\n",
        "\n",
        "3. Is there any relationship between the popularity and the duration of tracks? Explore this using a scatter plot?\n",
        "To explore the relationship between popularity and duration of tracks, we can use a scatter plot. This will allow us to visually assess whether there is any correlation between these two variables.\n",
        "\n",
        "Steps to Create the Scatter Plot:\n",
        "Extract the Popularity and Duration Columns: Focus on the Popularity and Duration (ms) columns.\n",
        "Plot a Scatter Plot: Use a scatter plot to visualize the relationship between popularity and duration.\n",
        "Python Code to Create the Scatter Plot:\n",
        "python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing values in 'Popularity' and 'Duration (ms)')\n",
        "df_cleaned = df.dropna(subset=['Popularity', 'Duration (ms)'])\n",
        "\n",
        "# Step 2: Plot the relationship between popularity and duration using a scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df_cleaned['Duration (ms)'], df_cleaned['Popularity'], alpha=0.5, color='orange')\n",
        "plt.title('Relationship Between Popularity and Duration of Tracks')\n",
        "plt.xlabel('Duration (ms)')\n",
        "plt.ylabel('Popularity')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "Explanation:\n",
        "Remove Missing Values: We drop any rows where either Popularity or Duration (ms) is missing using dropna(subset=['Popularity', 'Duration (ms)']).\n",
        "Plot Scatter Plot:\n",
        "plt.scatter() creates a scatter plot with Duration (ms) on the x-axis and Popularity on the y-axis.\n",
        "alpha=0.5 makes the points semi-transparent to better visualize dense areas.\n",
        "color='orange' sets the color of the points on the plot.\n",
        "Labels and Title: We add a title, x-label, and y-label for clarity.\n",
        "Example Output:\n",
        "The scatter plot will display the tracks as points, where:\n",
        "\n",
        "The x-axis shows the Duration (ms) of the tracks (how long each track is).\n",
        "The y-axis shows the Popularity score (how popular each track is).\n",
        "\n",
        "Steps to Achieve This:\n",
        "Count the Number of Tracks per Artist: Group the dataset by Artist and count the number of tracks for each artist.\n",
        "Plot the Count of Tracks per Artist: Use a countplot to visualize the number of tracks for each artist.\n",
        "Python Code to Create the Countplot:\n",
        "python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing 'Artist' or 'Track Name')\n",
        "df_cleaned = df.dropna(subset=['Artist', 'Track Name'])\n",
        "\n",
        "# Step 2: Count the number of tracks for each artist\n",
        "artist_track_count = df_cleaned['Artist'].value_counts()\n",
        "\n",
        "# Step 3: Plot the count of tracks for each artist using a countplot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(y='Artist', data=df_cleaned, order=artist_track_count.index, palette='viridis')\n",
        "plt.title('Number of Tracks per Artist')\n",
        "plt.xlabel('Number of Tracks')\n",
        "plt.ylabel('Artist')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Display the artist with the highest number of tracks\n",
        "most_tracks_artist = artist_track_count.idxmax()\n",
        "most_tracks_count = artist_track_count.max()\n",
        "\n",
        "print(f\"Artist with the highest number of tracks: {most_tracks_artist}\")\n",
        "print(f\"Number of tracks: {most_tracks_count}\")\n",
        "Explanation:\n",
        "Clean Data: We drop rows with missing Artist or Track Name values using dropna().\n",
        "Count Tracks per Artist: We use value_counts() on the Artist column to get the number of tracks per artist.\n",
        "Plot Countplot: We use Seaborn's countplot() to visualize the distribution of tracks across artists.\n",
        "y='Artist' specifies the artist names to be plotted on the y-axis.\n",
        "order=artist_track_count.index ensures that the artists are ordered based on the number of tracks.\n",
        "palette='viridis' is used to apply a color palette to the plot.\n",
        "Identify Artist with Most Tracks: We use .idxmax() to find the artist with the highest number of tracks and .max() to get the number of tracks they have.\n",
        "\n",
        "To identify the top 5 least popular tracks in the dataset, we need to sort the dataset based on the Popularity column in ascending order and then select the top 5 rows with the lowest popularity scores.\n",
        "\n",
        "Steps to Find the Top 5 Least Popular Tracks:\n",
        "Sort the dataset by Popularity in ascending order.\n",
        "Select the top 5 tracks with the lowest popularity scores.\n",
        "Display the Artist Name and Track Name for each of the least popular tracks.\n",
        "Python Code to Find the Top 5 Least Popular Tracks:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing popularity values)\n",
        "df_cleaned = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Step 2: Sort the dataset by popularity in ascending order\n",
        "least_popular_tracks = df_cleaned.sort_values(by='Popularity', ascending=True)\n",
        "\n",
        "# Step 3: Select the top 5 least popular tracks\n",
        "top_5_least_popular = least_popular_tracks[['Artist', 'Track Name', 'Popularity']].head(5)\n",
        "\n",
        "# Display the result\n",
        "print(\"Top 5 Least Popular Tracks:\")\n",
        "print(top_5_least_popular)\n",
        "Explanation:\n",
        "Remove Missing Popularity Values: We use dropna(subset=['Popularity']) to remove any rows with missing popularity values.\n",
        "Sort by Popularity: The dataset is sorted in ascending order based on the Popularity column using sort_values(by='Popularity', ascending=True).\n",
        "Select Top 5: The .head(5) function selects the first 5 rows, which correspond to the least popular tracks.\n",
        "Display Result: The Artist, Track Name, and Popularity columns are displayed for the top 5 least popular tracks.\n",
        "\n",
        "6. Among the top 5 most popular artists, which artist has the highest popularity on average? Calculate and display the average popularity for each artist?\n",
        "To calculate and display the average popularity for each of the top 5 most popular artists, we need to:\n",
        "\n",
        "Identify the top 5 most popular artists based on the average popularity of their tracks.\n",
        "Calculate the average popularity for each of these artists.\n",
        "Display the results.\n",
        "Steps to Achieve This:\n",
        "Group the dataset by Artist: Group the dataset by Artist and calculate the mean popularity for each artist.\n",
        "Sort the artists by average popularity: Sort the artists based on their average popularity in descending order to identify the top 5.\n",
        "Display the average popularity for each artist.\n",
        "Python Code to Calculate and Display the Average Popularity for the Top 5 Most Popular Artists:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing popularity values)\n",
        "df_cleaned = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Step 2: Group by Artist and calculate the average popularity for each artist\n",
        "average_popularity_by_artist = df_cleaned.groupby('Artist')['Popularity'].mean()\n",
        "\n",
        "# Step 3: Sort artists by average popularity in descending order and get the top 5\n",
        "top_5_most_popular_artists = average_popularity_by_artist.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Step 4: Display the average popularity for each of the top 5 most popular artists\n",
        "print(\"Average Popularity for the Top 5 Most Popular Artists:\")\n",
        "print(top_5_most_popular_artists)\n",
        "Explanation:\n",
        "Remove Missing Popularity Values: The dataset is cleaned to remove rows with missing Popularity values using dropna(subset=['Popularity']).\n",
        "Group by Artist: We use groupby('Artist') to group the data by Artist, then calculate the mean Popularity for each artist with .mean().\n",
        "Sort and Select Top 5: The artists are sorted by their average popularity in descending order using sort_values(ascending=False), and the top 5 artists are selected using .head(5).\n",
        "Display Results: The average popularity for each of the top 5 artists is printed.\n",
        "\n",
        "7. For the top 5 most popular artists, what are their most popular tracks? List the track name for each artist?\n",
        "To find the most popular tracks for the top 5 most popular artists in the dataset, we need to:\n",
        "\n",
        "Identify the top 5 most popular artists based on their average popularity.\n",
        "Filter the dataset for tracks by these artists.\n",
        "For each artist, identify the track with the highest popularity.\n",
        "Display the artist name along with their most popular track.\n",
        "Steps:\n",
        "Calculate the Average Popularity for each artist and identify the top 5.\n",
        "For each of the top 5 artists, find the track with the highest popularity.\n",
        "Display the results.\n",
        "Python Code to Find the Most Popular Track for Each of the Top 5 Artists:\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing popularity values)\n",
        "df_cleaned = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Step 2: Group by Artist and calculate the average popularity for each artist\n",
        "average_popularity_by_artist = df_cleaned.groupby('Artist')['Popularity'].mean()\n",
        "\n",
        "# Step 3: Sort artists by average popularity in descending order and get the top 5\n",
        "top_5_most_popular_artists = average_popularity_by_artist.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Step 4: For each top 5 artist, find the most popular track\n",
        "most_popular_tracks = {}\n",
        "\n",
        "for artist in top_5_most_popular_artists.index:\n",
        "    # Filter tracks by artist\n",
        "    artist_tracks = df_cleaned[df_cleaned['Artist'] == artist]\n",
        "    # Find the track with the highest popularity for this artist\n",
        "    most_popular_track = artist_tracks.loc[artist_tracks['Popularity'].idxmax(), 'Track Name']\n",
        "    most_popular_tracks[artist] = most_popular_track\n",
        "\n",
        "# Step 5: Display the most popular tracks for the top 5 artists\n",
        "print(\"Most Popular Tracks for the Top 5 Most Popular Artists:\")\n",
        "for artist, track in most_popular_tracks.items():\n",
        "    print(f\"Artist: {artist}, Most Popular Track: {track}\")\n",
        "Explanation:\n",
        "Calculate Average Popularity: The average popularity for each artist is calculated by grouping by Artist and using the .mean() function.\n",
        "Identify Top 5 Artists: The artists are sorted based on their average popularity in descending order using sort_values(), and the top 5 artists are selected with .head(5).\n",
        "Find Most Popular Track:\n",
        "For each of the top 5 artists, we filter the dataset to include only their tracks.\n",
        "We use .idxmax() to find the index of the track with the highest popularity, and retrieve the corresponding Track Name.\n",
        "Display Results: The artist name and their most popular track are printed.\n",
        "\n",
        "A pair plot is a great way to visualize relationships between multiple numerical variables simultaneously. It displays scatter plots between each pair of variables in the dataset, along with histograms or density plots on the diagonal for each variable's distribution. This is useful for identifying correlations and patterns.\n",
        "\n",
        "Steps to Create a Pair Plot:\n",
        "Select Numerical Columns: We'll select the relevant numerical columns for the pair plot. Based on your dataset, these could be Popularity, Duration (ms), or any other numerical variables you might have.\n",
        "Use Seaborn's pairplot(): We can use Seaborn's pairplot() to generate this visualization.\n",
        "Python Code to Create a Pair Plot:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing values in numerical columns)\n",
        "df_cleaned = df.dropna(subset=['Popularity', 'Duration (ms)'])\n",
        "\n",
        "# Step 2: Select the relevant numerical columns for the pair plot\n",
        "numerical_columns = ['Popularity', 'Duration (ms)']\n",
        "\n",
        "# Step 3: Create the pair plot\n",
        "sns.pairplot(df_cleaned[numerical_columns])\n",
        "plt.suptitle('Pair Plot of Popularity and Duration (ms)', y=1.02)  # Add a title above the plot\n",
        "plt.show()\n",
        "Explanation:\n",
        "Clean Data: We remove any rows with missing values in Popularity or Duration (ms) using dropna().\n",
        "Select Numerical Columns: In this example, we're visualizing the relationship between Popularity and Duration (ms). You can add more numerical columns to the numerical_columns list if desired.\n",
        "Create Pair Plot: We use Seaborn's pairplot() to visualize the pairwise relationships between the selected numerical columns.\n",
        "The diagonal of the plot will show histograms (distribution of each variable).\n",
        "The off-diagonal will show scatter plots (relationship between the pairs of variables).\n",
        "Title: We add a title to the plot with plt.suptitle().\n",
        "\n",
        "9.Does the duration of tracks vary significantly across different artists? Explore this visually using a box plot or violin plot?\n",
        "To explore whether the duration of tracks varies significantly across different artists, a box plot or violin plot would be ideal. These plots help to visualize the distribution of track durations for each artist, and how much variability exists within each group.\n",
        "\n",
        "Steps for Visualization:\n",
        "Prepare the Data: Group the data by Artist and examine the Duration (ms) for each.\n",
        "Create a Box Plot or Violin Plot: These plots will show the distribution of track durations for each artist, including the median, quartiles, and any outliers.\n",
        "Python Code to Create a Box Plot or Violin Plot:\n",
        "Box Plot:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing duration values)\n",
        "df_cleaned = df.dropna(subset=['Duration (ms)'])\n",
        "\n",
        "# Step 2: Create a box plot to explore the distribution of track durations by artist\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Artist', y='Duration (ms)', data=df_cleaned)\n",
        "plt.xticks(rotation=90)  # Rotate artist names for better readability\n",
        "plt.title('Track Duration Distribution Across Different Artists')\n",
        "plt.show()\n",
        "Violin Plot:\n",
        "python\n",
        "# Create a violin plot to explore the distribution of track durations by artist\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='Artist', y='Duration (ms)', data=df_cleaned)\n",
        "plt.xticks(rotation=90)  # Rotate artist names for better readability\n",
        "plt.title('Track Duration Distribution Across Different Artists (Violin Plot)')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Clean Data: We remove rows with missing Duration (ms) values using dropna().\n",
        "Box Plot: The box plot will show:\n",
        "The median track duration (the central line in each box).\n",
        "The interquartile range (IQR) (the box), representing the middle 50% of the data.\n",
        "The whiskers (lines outside the box) show the range of the data, except for outliers, which are shown as individual points.\n",
        "Violin Plot: The violin plot is similar but adds a density plot on each side of the box, showing the distribution's shape. It's useful for comparing the distribution across different artists.\n",
        "Rotating X-axis Labels: plt.xticks(rotation=90) ensures the artist names are readable.\n",
        "\n",
        "To explore how the distribution of track popularity varies across different artists, both a swarm plot and a violin plot would be excellent choices. These plots will help visualize the distribution and patterns in popularity for each artist, highlighting the variation and any outliers.\n",
        "\n",
        "Steps for Visualization:\n",
        "Prepare the Data: Weâ€™ll focus on the Popularity and Artist columns.\n",
        "Create a Swarm Plot or Violin Plot: These plots will help show how track popularity is distributed for each artist.\n",
        "Python Code for Visualization:\n",
        "Swarm Plot:\n",
        "python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace 'spotify_hiphop_data.csv' with your actual file path)\n",
        "df = pd.read_csv('spotify_hiphop_data.csv')\n",
        "\n",
        "# Step 1: Clean data (remove rows with missing popularity values)\n",
        "df_cleaned = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Step 2: Create a swarm plot to visualize track popularity distribution for different artists\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.swarmplot(x='Artist', y='Popularity', data=df_cleaned, jitter=True, size=5)\n",
        "plt.xticks(rotation=90)  # Rotate artist names for better readability\n",
        "plt.title('Distribution of Track Popularity for Different Artists')\n",
        "plt.show()\n",
        "Violin Plot:\n",
        "python\n",
        "# Create a violin plot to visualize track popularity distribution for different artists\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='Artist', y='Popularity', data=df_cleaned)\n",
        "plt.xticks(rotation=90)  # Rotate artist names for better readability\n",
        "plt.title('Distribution of Track Popularity for Different Artists (Violin Plot)')\n",
        "plt.show()\n",
        "Explanation:\n",
        "Clean Data: We remove rows with missing Popularity values using dropna().\n",
        "Swarm Plot: This plot places each data point as a dot along the vertical axis (representing Popularity) for each artist. The jitter=True argument ensures the points are spread out horizontally, preventing overlap. Itâ€™s particularly useful for visualizing the density of popularity values and spotting any outliers.\n",
        "Violin Plot: This plot shows the distribution of Popularity for each artist. The \"violin\" shape represents the kernel density estimate (KDE) of the popularity data, and the width of the plot at each level indicates how frequently values appear in that range. It also includes a marker for the median and interquartile range.\n",
        "Rotating X-axis Labels: plt.xticks(rotation=90) ensures the artist names are legible if they are long.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}